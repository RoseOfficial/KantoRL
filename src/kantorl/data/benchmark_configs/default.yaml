# KantoRL Benchmark Configuration
#
# This file defines configurations for benchmark comparisons.
# Run with: kantorl benchmark run default.yaml pokemon_red.gb
#
# Structure:
#   benchmark: Global benchmark settings
#   configs: Named configurations to compare (each is a dict of KantoConfig overrides)
#
# Tip: Start with bronze tier and low max_steps for fast iteration,
#      then increase for publication-quality results.

# =============================================================================
# BENCHMARK SETTINGS
# =============================================================================
benchmark:
  # Milestone tier: bronze (1 badge), silver (4 badges), gold (8 badges), champion (E4)
  tier: bronze

  # Maximum training steps per configuration
  # Bronze typically needs 500K-2M steps, Silver needs 5-20M, Gold needs 20-100M
  max_steps: 2_000_000

  # Number of parallel environments (more = faster but more RAM)
  n_envs: 16

  # Random seeds for statistical validity (run each config multiple times)
  # Using 3 seeds gives reasonable variance estimates
  seeds: [42, 123, 456]

  # Stop training when milestone is reached (vs running to max_steps)
  early_stop: true

  # Evaluation episodes for checkpoint evaluation (not used in training mode)
  eval_episodes: 10

# =============================================================================
# CONFIGURATIONS TO COMPARE
# =============================================================================
# Each configuration is a set of overrides to KantoConfig.
# Empty dict {} uses all defaults.
#
# Available overrides (see config.py for full list):
#   - learning_rate: float (default: 3e-4)
#   - gamma: float (default: 0.997)
#   - n_steps: int (default: 64)
#   - batch_size: int (default: 256)
#   - ent_coef: float (default: 0.01)
#   - gae_lambda: float (default: 0.95)
#   - clip_range: float (default: 0.1)
#   - reward_scale: float (default: 0.5)
#   - explore_weight: float (default: 0.1)

configs:
  # Baseline configuration (all defaults)
  baseline: {}

  # Higher learning rate - faster learning but potentially less stable
  high_lr:
    learning_rate: 1e-3

  # Lower learning rate - slower but potentially more stable
  low_lr:
    learning_rate: 1e-4

  # More exploration - higher entropy bonus
  high_explore:
    ent_coef: 0.05
    explore_weight: 0.3

  # Longer rollouts - better advantage estimation but slower updates
  long_rollout:
    n_steps: 128
    batch_size: 512

# =============================================================================
# OPTUNA SEARCH SPACE (for hyperparameter optimization)
# =============================================================================
# Used when running: kantorl benchmark search pokemon_red.gb
# Each parameter defines type and range for sampling.

search_space:
  # Learning rate (log-uniform between 1e-5 and 1e-2)
  learning_rate:
    type: float
    low: 1e-5
    high: 1e-2
    log: true

  # Discount factor (uniform between 0.95 and 0.999)
  gamma:
    type: float
    low: 0.95
    high: 0.999

  # Steps per rollout (categorical)
  n_steps:
    type: categorical
    choices: [32, 64, 128, 256]

  # Entropy coefficient (log-uniform)
  ent_coef:
    type: float
    low: 0.001
    high: 0.1
    log: true

  # Exploration weight (log-uniform)
  explore_weight:
    type: float
    low: 0.01
    high: 0.5
    log: true
